module MyLearn2Classify

using IJulia, Plots
using Random: seed!, randperm

# these are the functions that will be immediately accessible after `using MyLearn2Classify`
export linear, dlinear, dtanh, sigmoid, dsigmoid, gn, grad_loss, learn2classify_asgd

# Activation functions
linear(z) = z
dlinear(z) = 1.0

# tanh is already defined in Julia
dtanh(z) = 1 - tanh(z)^2

sigmoid(z::Real) = 1.0 / (1.0 + exp(-z))
function dsigmoid(z::Real)
    sigmoid_z = 1.0 / (1.0 + exp(-z))
    return sigmoid_z * (1 - sigmoid_z)
end

function gn(x::AbstractArray, w::AbstractVector, b::Number, f_a::Function)
    return f_a.(x' * w .+ b)
end


# TODO: complete the training functions below
function grad_loss(
        f_a::Function,
        df_a::Function,
        x::AbstractMatrix,
        y::AbstractVector,
        w::AbstractVector,
        b::Number,
        normalize::Bool=true
    )  

    n, d = size(w)
    N = size(y, 2) # assume y is matrix of size n x N
    
    dW = zeros(n, d) 
    db = zeros(n)
    loss = 0.0

    for k in 1:N
        for p in 1:n
            error = y[p, k] - f_a(W[p, :]' * x[:, k] + b[p])
            common_term = error * df_a(W[p, :]' * x[:, k] + b[p])
            for q in 1:d
                dW[p, q] = dW[p, q] - 2 / N * common_term * x[q, k]
            end
            db[p] = db[p] - 2 / N * common_term
            loss += 1 / N * error^2
        end
    end

    return dW, db, loss
 end

function learn2classify_asgd(
        f_a::Function,
        df_a::Function,
        grad_loss::Function,
        x::AbstractMatrix,
        y::AbstractVector,
        mu::Number=1e-3,
        iters::Integer=500,
        batch_size::Integer=10,
        show_loss::Bool=true,
        normalize::Bool=true,
        seed::Integer=1
    )

    

    (seed != 0) && seed!(seed) # use seed if provided

    d = size(x, 1) #number of inputs
    n = 8 # number of neurons
    N = size(x, 2) # number of training samples
 
    W = rand(n, d)
    b = rand(d)
    
    loss = zeros(iters)

    lambda_k = 0
    q_k = W
    p_k = b
    for i in 1:iters
        batch_idx = randperm(N)
        batch_idx = batch_idx[1:min(batch_size, N)]
        
        dW, db, loss_i = grad_loss(f_a, df_a, x[:, batch_idx], y[:, batch_idx], W, b)
        
        q_kp1 = W - mu * dW
        p_kp1 = b - mu * db
        
        lambda_kp1 = (1 + sqrt(1 + 4 * lambda_k^2)) / 2
        gamma_k = (1 - lambda_k) / lambda_kp1
        W = (1 - gamma_k) * q_kp1 + gamma_k * q_k
        b = (1 - gamma_k) * p_kp1 + gamma_k * p_k

        q_k = q_kp1
        p_k = p_kp1
        lambda_k = lambda_kp1

        loss[i] = loss_i
    end

    return w, b, loss
end

end # module